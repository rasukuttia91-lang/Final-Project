# -*- coding: utf-8 -*-
"""finalproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CdRe1s4pSpSerrDjPxKN0ajYgDu5pbZ0

Project Name : Advanced Time Series Forecasting using LSTM with Attention

1. Importing Libraries

First, we import all the required Python libraries for data processing, visualization, machine learning, and deep learning.
"""

# 1. Importing Libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import TimeSeriesSplit

import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Dense, Attention
from tensorflow.keras.models import Model

"""2. Synthetic Dataset Generation

A synthetic multivariate time series dataset is generated programmatically using NumPy.
"""

#2. Synthetic Dataset Generation

np.random.seed(42)
tf.random.set_seed(42)
n = 1500
t = np.arange(n)

trend = 0.01 * t
seasonal = 2 * np.sin(2 * np.pi * t / 50)

f1 = trend + seasonal + np.random.normal(0, 0.5, n)
f2 = 0.5 * f1 + np.random.normal(0, 0.3, n)
f3 = np.cos(t / 20) + np.random.normal(0, 0.2, n)
f4 = np.sin(t / 15) + np.random.normal(0, 0.2, n)
f5 = trend + np.random.normal(0, 0.4, n)

target = 0.3 * f1 + 0.2 * f2 + 0.4 * f3 + seasonal + np.random.normal(0, 0.3, n)

data = pd.DataFrame({
                      'f1' : f1,
                      'f2' : f2,
                      'f3' : f3,
                      'f4' : f4,
                      'f5' : f5,
                      'y' : target
                    })
data.head()

"""3. Data Visualization

The target time series is plotted using Matplotlib
"""

# 3 dagta visualization

plt.figure(figsize = (12, 7))
plt.plot(data['y'], label = 'target')
plt.title("Synthetic Time Series Target")
plt.legend()
plt.show()

"""Data Scaling

The entire dataset is standardized using StandardScaler.

Sequence Creation (Supervised Learning)

A sliding window approach is used to convert the time series into supervised learning format

Train-Test Split

This split preserves the temporal order of the data, which is essential in time series problems
"""

# 4 Scaling and Sequence Creation and Train Test Split

def create_sequences(data, seq_len = 30):
  x, y = [], []
  for i in range(len(data) - seq_len):
    x.append(data[i : i + seq_len, : -1])
    y.append(data[i + seq_len, -1])
  return np.array(x), np.array(y)

SEQ_LEN = 30

split_raw = int(0.8 * len(data))
train_raw = data[:split_raw]
test_raw = data[split_raw:]

scaler = StandardScaler()
train_scaled = scaler.fit_transform(train_raw)
test_scaled = scaler.transform(test_raw)

# Create sequences from the scaled data
x_train, y_train = create_sequences(train_scaled, SEQ_LEN)
x_test, y_test = create_sequences(test_scaled, SEQ_LEN)

""" LSTM with Attention Model

 A deep learning model is constructed using Keras
"""

# 5 LSTM and attention model

inputs = Input(shape = (SEQ_LEN, 5))
lstm_out = LSTM(64, return_sequences = True)(inputs)

attn = Attention(name = "attention_layer")([lstm_out, lstm_out])
context = tf.keras.ops.mean(attn, axis = 1)

output = Dense(1)(context)

model = Model(inputs , output)
model.compile(optimizer = "adam", loss = "mse")
model.summary()

"""Rolling Time Series Cross-Validation

Rolling cross-validation is performed using TimeSeriesSplit
"""

# 6 rolling cross validation

tscv = TimeSeriesSplit(n_splits = 5)

for fold, (train_idx, val_idx) in enumerate(tscv.split(x_train)):
  print(f"Fold {fold + 1}")
  x_tr, x_val = x_train[train_idx], x_train[val_idx]
  y_tr, y_val = y_train[train_idx], y_train[val_idx]

  model.fit(x_tr, y_tr,
            validation_data = (x_val, y_val),
            epochs = 10,
            batch_size = 32,
            verbose = 0)

"""Final Model Training

After cross-validation, the model is trained on the full training dataset
"""

# 7 final training

history = model.fit(x_train, y_train,
                    epochs = 20,
                    batch_size = 32,
                    verbose = 0,
                    validation_split = 0.01)

"""Deep Learning Model Evaluation

Predictions are generated on the test set
"""

# 8 deeplearning evalution

y_pred_d1 = model.predict(x_test)

rmse_d1 = np.sqrt(mean_squared_error(y_test, y_pred_d1))
mae_d1 = mean_absolute_error(y_test, y_pred_d1)
mape_d1 = np.mean(np.abs((y_test - y_pred_d1.flatten()) / y_test)) * 100

print("LSTM + Attention:\n")
print("RMSE:L\n", rmse_d1)
print("MAE:\n", mae_d1)
print("MAPE:\n", mape_d1)

"""Baseline Model (SARIMA)

A classical SARIMA model is trained on the same target time series
"""

# 9 base line SARIMA modelt

from statsmodels.tsa.statespace.sarimax import SARIMAX

train_series = data['y'][:split_raw + SEQ_LEN]
test_series = data['y'][split_raw + SEQ_LEN:]

sarima = SARIMAX(train_series, order = (3, 1, 3))
sarima_fit = sarima.fit()

sarima_pred = sarima_fit.forecast(len(test_series))

rmse_s = np.sqrt(mean_squared_error(test_series, sarima_pred))
mae_s = mean_absolute_error(test_series, sarima_pred)
mape_s = np.mean(np.abs((test_series - sarima_pred) / test_series)) * 100

print("SARIMA:\n")
print("RMSE:\n", rmse_s)
print("MAE:\n", mae_s)
print("MAPE:\n", mape_s)

"""Results Comparison

A comparison table is created using Pandas
"""

# 10 result comparison table

results = pd.DataFrame({
    "Mode1" : ["SARIMA", "LSTM + Attention"],
    "RMSE" : [rmse_s, rmse_d1],
    "MAE" : [mae_s, mae_d1],
    "MAPE" : [mape_s, mape_d1]
})

results

"""Attention Weights Extraction

A separate model is created to extract the output of the attention layer
"""

# 11 attention weights extraction

attn_model = Model(inputs = model.input,
                   outputs = model.get_layer("attention_layer").output
                   )
weights = attn_model.predict(x_test[:1])
print(weights.shape)

"""Attention Visualization

The attention weights are visualized using a heatmap
"""

# 12 attention visualization

plt.figure(figsize = (18,18))
sns.heatmap(weights[0], cmap = "viridis", annot = True, fmt = ".2f")
plt.title("Attention Weights Heatmap")
plt.xlabel("Time Steps")
plt.ylabel("Time Steps")
plt.show()

"""Final Prediction Plot"""

# 13 final plot

plt.figure(figsize = (10, 5))
plt.plot(y_test[:200], label = "Actual")
plt.plot(y_pred_d1[:200], label = "Predicted")
plt.title("Time Series Prediction")
plt.legend()
plt.show()